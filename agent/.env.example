# ==============================================================================
# nousflash Environment Configuration Template
# Copy this file to .env and fill in your values
# ==============================================================================

# --- Twitter / X ---
# OAuth 1.0a credentials (from developer.twitter.com)
X_CONSUMER_KEY=""
X_CONSUMER_SECRET=""
X_ACCESS_TOKEN=""
X_ACCESS_TOKEN_SECRET=""

# Account credentials for cookie-based auth (twitter-api-client)
X_USERNAME=""
X_EMAIL=""
X_PASSWORD=""

# Cookie auth tokens as JSON (e.g. {"auth_token": "...", "ct0": "..."})
X_AUTH_TOKENS=""

# --- LLM API Keys ---
# Hyperbolic API (base model completions)
HYPERBOLIC_API_KEY=""

# OpenAI (used for embeddings and formatting)
OPENAI_API_KEY=""

# OpenRouter (fallback/additional models)
OPENROUTER_API_KEY=""

# Anthropic API (Claude Opus / Sonnet / Haiku)
# Get key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=""

# --- Anthropic Inference Parameters ---
# Only used when INFERENCE_MODE=anthropic
#
# Model selection — pick one:
#   claude-opus-4-5-20250929   (most capable, highest cost)
#   claude-sonnet-4-5-20250929 (balanced)
#   claude-3-5-haiku-20241022  (fastest, lowest cost)
ANTHROPIC_MODEL="claude-opus-4-5-20250929"

# Maximum tokens to generate per response (1–4096 for Opus; up to 8192 for Sonnet)
ANTHROPIC_MAX_TOKENS=512

# Sampling temperature: 0.0 = deterministic, 1.0 = max randomness
# Anthropic range: 0.0–1.0  (default 1.0)
ANTHROPIC_TEMPERATURE=1.0

# Nucleus sampling — only tokens comprising top P% of probability mass are sampled
# Range: 0.0–1.0  (mutually exclusive with top_k; leave blank to let model use default)
ANTHROPIC_TOP_P=0.95

# Top-k sampling — limit next-token candidates to k highest-probability tokens
# Range: integer ≥ 1  (mutually exclusive with top_p; leave blank to disable)
# ANTHROPIC_TOP_K=40

# Formatting model — second-pass cleanup/extraction step
# Can be a cheaper/faster model than the base generation model
ANTHROPIC_FORMATTER_MODEL="claude-3-5-haiku-20241022"

# System prompt for the base generation step (tweet creation)
# Uses the tweet persona from prompts.py by default; override here if needed
# ANTHROPIC_SYSTEM_PROMPT=""

# --- Ethereum / Wallet ---
# Public RPC endpoint for Ethereum mainnet
# Free options: https://cloudflare-eth.com, https://rpc.ankr.com/eth
ETH_MAINNET_RPC_URL="https://cloudflare-eth.com"

# --- Database ---
# SQLite path (inside container: /data/agents.db)
SQLITE_DB_PATH=/data/agents.db

# --- Inference Backend ---
# Controls which LLM backend is used for tweet generation
#   api        — Hyperbolic API (Meta-Llama-3.1-405B + 70B formatter) [default]
#   anthropic  — Anthropic Claude (model set by ANTHROPIC_MODEL above)
#   local      — Xortron2025 local GGUF via llama.cpp (requires 21GB+ RAM)
INFERENCE_MODE=api

# --- Tweet Prompt (optional) ---
# Override the default tweet generation prompt.
# Must use Python .format() placeholders:
#   {short_term_memory}, {long_term_memories}, {recent_posts},
#   {external_context}, {example_tweets}
# Leave unset to use the built-in default from prompts.py.
# TWEET_PROMPT_TEMPLATE=""

# --- News API (optional) ---
# For external context fetching
# NEWS_API_KEY=""
